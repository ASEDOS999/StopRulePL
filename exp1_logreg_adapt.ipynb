{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#import copy\n",
    "#import time\n",
    "#from collections import defaultdict\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import gradf_inexact\n",
    "from methods import GradientDescent, parse_logs, AdaptiveNoiseGD\n",
    "from methods import ConstantStepSize, AdaptiveL, AdaptiveLdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "params = {'legend.fontsize': 20,\n",
    "          'legend.handlelength': 4,\n",
    "          \"axes.labelsize\": 45,\n",
    "          \"xtick.labelsize\": 25,\n",
    "          \"ytick.labelsize\": 25,\n",
    "          \"lines.linewidth\": 2,\n",
    "           \"axes.titlesize\":30}\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pics = \"../pics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(w, X, Y, sigma=0):\n",
    "    logit = -Y * (X@w)\n",
    "    logit1 = jnp.clip(logit, None, 0)\n",
    "    logit2 = jnp.clip(logit, 0, None)\n",
    "    log = jnp.log(jnp.exp(logit1)+jnp.exp(-logit2)) + logit2\n",
    "    return (log).sum() / len(Y) + sigma * (w**2).sum()\n",
    "\n",
    "gradf = jax.grad(f1, argnums=0, has_aux=False)\n",
    "jit_gradf = jax.jit(gradf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataset\n",
    "\n",
    "Creating of such dataset that logistic regression without regularization has finite solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110, 200), (110,), (200,), 15.744252812419115)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n, m = 100, 200\n",
    "k = 10\n",
    "X = np.random.randn(n, m)\n",
    "X = np.vstack([X, X[-k:]])\n",
    "w = np.random.randn(m)\n",
    "Y = np.sign(X @ w)\n",
    "Y[-k:] *= -1\n",
    "X.shape, Y.shape, w.shape, np.linalg.norm(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 200), (700,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n, m = 700, 200\n",
    "k = 50\n",
    "\n",
    "W, _ = np.linalg.qr(np.random.randn(m, m))\n",
    "#W = np.eye(m)\n",
    "X_base = W[:k]\n",
    "X_w = np.random.randint(-10, 10, (n-2*k, k))\n",
    "#X_w = (X_w.T / np.linalg.norm(X_w, 2, 1)).T\n",
    "X = np.vstack([X_base, X_base, X_w @ X_base])\n",
    "X = (X - X.mean(0))/X.std()\n",
    "w = np.random.randn(m)\n",
    "Y = np.sign(X@w)\n",
    "Y[:k] *= -1\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5794450694715507"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X\n",
    "eigvals, _ = np.linalg.eigh(A.T @ A)\n",
    "L = np.real((eigvals.max()) / 4 / A.shape[0])\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=0\n",
    "gradf = lambda x: np.array(jit_gradf(x, X, Y, sigma).block_until_ready())\n",
    "f = lambda x: f1(x, X, Y, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1374615491296541, 0.6931471805599454)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w = np.random.randn(X.shape[-1])\n",
    "f(w).item(), f(np.zeros(m)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Noise Distributed on the Unit Sphere\n",
    "\n",
    "The case when $\\xi \\sim \\mathcal{U}(S_1(0))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1e-05\t902\t2856.98\t13.392339\t2.22\n",
      "\t1e-05\t23\t449.68\t13.414217\t3.37\n",
      "\t1e-05\t20002\t5604.34\t13.342222\t3.56\n",
      "\n",
      "\t0.0001\t472\t1678.02\t12.753213\t2.16\n",
      "\t0.0001\t25\t370.09\t12.908592\t3.62\n",
      "\t0.0001\t9700\t2605.02\t12.655907\t2.42\n",
      "\n",
      "\t0.01\t17\t68.36\t3.846313\t2.10\n",
      "\t0.01\t17\t161.27\t5.597237\t0.84\n",
      "\t0.01\t83\t49.86\t3.468000\t2.29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "mu_list = [0]\n",
    "res = {mu:{\"delta\":[], \n",
    "           \"iters_adaptL\":[], \"time_adaptL\":[], \"adaptL,x0-x*\": [], \"normg_adaptL\": [], \"residual_adaptL\":[],\n",
    "           \"iters_exact\":[], \"time_exact\":[], \"exact,x0-x*\": [], \"normg_exact\": [], \"residual_exact\":[], \n",
    "          \"iters_adaptLdelta\":[], \"time_adaptLdelta\":[], \"adaptLdelta,x0-x*\": [], \"normg_adaptLdelta\": []} for mu in mu_list}\n",
    "dist = {}\n",
    "number = 1\n",
    "\n",
    "Delta_list = [1e-5, 1e-4, 1e-2]\n",
    "N = int(2e4)\n",
    "\n",
    "for mu in mu_list:\n",
    "    w = np.random.randn(m)\n",
    "\n",
    "    alpha = 1/L\n",
    "    w = np.ones(m)*0.1\n",
    "\n",
    "    v = np.random.randn(*w.shape)\n",
    "    v = np.ones(*w.shape)\n",
    "    save_iter = int(1)\n",
    "    tol = 1e-9\n",
    "    methods = []\n",
    "    for Delta in Delta_list:\n",
    "        eps = Delta**2 / 16\n",
    "        f2 = lambda x: f(x) + eps*np.random.uniform(-1, 1)\n",
    "        res[mu][\"delta\"].append(int(np.log10(Delta)))\n",
    "        tol = np.sqrt(6)*Delta\n",
    "\n",
    "        grad_inexact = lambda w: gradf_inexact(w, gradf, Delta, 1, v=v)\n",
    "        method = GradientDescent(AdaptiveL(L0=1, Delta=Delta, Lmin=mu/4, delta=eps), name=\"GD, Delta={}\".format(Delta), save_iter=save_iter)\n",
    "        x = method.solve(w, f2, grad_inexact, tol=tol, max_iter=N)\n",
    "        g = lambda: GradientDescent(AdaptiveL(L0=1, Delta=Delta, Lmin=mu/4, delta=eps),\n",
    "                                    return_history=False).solve(w, f2, grad_inexact, tol=tol, max_iter=N)\n",
    "        T = timeit.timeit(g, number=number)/number        \n",
    "        print(\"\\t{}\\t{}\\t{:.2f}\\t{:.6f}\\t{:.2f}\".format(Delta, len(method.history), T*1000, np.linalg.norm(x-w), \n",
    "                                                np.linalg.norm(gradf(x))/Delta))\n",
    "        methods.append(method)\n",
    "        res[mu][\"iters_adaptL\"].append(len(method.history))\n",
    "        res[mu][\"time_adaptL\"].append(\"{:.2f}\".format(T*1000))\n",
    "        res[mu][\"adaptL,x0-x*\"].append(\"{:.1f}\".format(np.linalg.norm(x-w)))\n",
    "        res[mu][\"normg_adaptL\"].append(\"{:.2f}\".format(np.linalg.norm(gradf(x))/Delta))\n",
    "        res[mu][\"residual_adaptL\"].append(f(x))\n",
    "\n",
    "        Lmin = mu/4\n",
    "        \n",
    "        method = AdaptiveNoiseGD(AdaptiveLdelta(L0=1, mindelta=1e-12, Lmin=Lmin, mu=mu, delta_alpha=2), \n",
    "                                 name=\"GD, Delta={}\".format(Delta), save_iter=save_iter, alpha=np.sqrt(6))\n",
    "        x = method.solve(w, f, grad_inexact, max_iter=N)\n",
    "        g = lambda: method.solve(w, f, grad_inexact, max_iter=N)\n",
    "        T = timeit.timeit(g, number=number)/number        \n",
    "        #T = 0\n",
    "        print(\"\\t{}\\t{}\\t{:.2f}\\t{:.6f}\\t{:.2f}\".format(Delta, len(method.history), T*1000, np.linalg.norm(x-w), \n",
    "                                                np.linalg.norm(gradf(x))/Delta))\n",
    "        methods.append(method)\n",
    "        res[mu][\"iters_adaptLdelta\"].append(len(method.history))\n",
    "        res[mu][\"time_adaptLdelta\"].append(\"{:.2f}\".format(T*1000))\n",
    "        res[mu][\"adaptLdelta,x0-x*\"].append(\"{:.1f}\".format(np.linalg.norm(x-w)))\n",
    "        res[mu][\"normg_adaptLdelta\"].append(\"{:.2f}\".format(np.linalg.norm(gradf(x))/Delta))        \n",
    "\n",
    "        method = GradientDescent(ConstantStepSize(alpha), name=\"GD, Delta={}\".format(Delta), save_iter=save_iter)\n",
    "        x = method.solve(w, f, grad_inexact, tol=tol, max_iter=N)\n",
    "        g = lambda: GradientDescent(ConstantStepSize(alpha),\n",
    "                                    return_history=False).solve(w, f, grad_inexact, tol=tol, max_iter=N)\n",
    "        T = timeit.timeit(g, number=number)/number\n",
    "        print(\"\\t{}\\t{}\\t{:.2f}\\t{:.6f}\\t{:.2f}\".format(Delta, len(method.history), T*1000, np.linalg.norm(x-w), \n",
    "                                                np.linalg.norm(gradf(x))/Delta))\n",
    "        methods.append(method)\n",
    "        res[mu][\"iters_exact\"].append(len(method.history))\n",
    "        res[mu][\"time_exact\"].append(\"{:.2f}\".format(T*1000))\n",
    "        res[mu][\"exact,x0-x*\"].append(\"{:.1f}\".format(np.linalg.norm(x-w)))\n",
    "        res[mu][\"normg_exact\"].append(\"{:.2f}\".format(np.linalg.norm(gradf(x))/Delta))\n",
    "        res[mu][\"residual_exact\"].append(f(x))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{@{}c@{}} $10^{-5}$ \\\\ $10^{-4}$ \\\\ $10^{-2}$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $20002$ \\\\ $9700$ \\\\ $83$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $5604.34$ \\\\ $2605.02$ \\\\ $49.86$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $3.56$ \\\\ $2.42$ \\\\ $2.29$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $902$ \\\\ $472$ \\\\ $17$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $2856.98$ \\\\ $1678.02$ \\\\ $68.36$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $2.22$ \\\\ $2.16$ \\\\ $2.10$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $23$ \\\\ $25$ \\\\ $17$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $449.68$ \\\\ $370.09$ \\\\ $161.27$ \\end{tabular}&\\begin{tabular}{@{}c@{}} $3.37$ \\\\ $3.62$ \\\\ $0.84$ \\end{tabular}\\\\\n",
      "\\hline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\n",
    "\n",
    "for mu in mu_list:\n",
    "    #s += str(mu) + \" & \"\n",
    "    cur_list = [\"$10^{{{}}}$\".format(i) for i in res[mu][\"delta\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    \n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"iters_exact\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"time_exact\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"normg_exact\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    \n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"iters_adaptL\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"time_adaptL\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"normg_adaptL\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    \n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"iters_adaptLdelta\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"time_adaptLdelta\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}&\"\n",
    "    cur_list = [\"${}$\".format(i) for i in res[mu][\"normg_adaptLdelta\"]]\n",
    "    s+= \"\\\\begin{tabular}{@{}c@{}} \" + \" \\\\\\\\ \".join(cur_list) + \" \\\\end{tabular}\"\n",
    "    \n",
    "    s+= \"\\\\\\\\\\n\\\\hline\\n\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
